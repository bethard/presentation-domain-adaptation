% !TEX TS-program = pdflatexmk

\documentclass[14pt]{beamer}
\usepackage{newtxtext,newtxmath}
\usepackage{microtype}
\usepackage[english]{babel}
\usepackage{array}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shadows}


% Define UA colors
% https://brand.arizona.edu/applying-the-brand/colors
\definecolor{ua-red}{HTML}{AB0520}
\definecolor{ua-blue}{HTML}{0C234B}
\definecolor{ua-oasis}{HTML}{378DBD}

\mode<presentation>{
\usetheme{Madrid}
\usecolortheme[named=ua-red]{structure}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{subsection in toc}[square]
\setbeamertemplate{items}[square]
\setbeamercovered{transparent=0}
}

\author[Bethard]{Dr. Steven Bethard}
\institute[Arizona]{%
School of Information\\
University of Arizona}

\title{Adapting natural language processing models across clinical domains}
\date[]{26 Feb 2021}


\begin{document}


\begin{frame}
  \titlepage
\end{frame}

\section{Machine learning needs domain adaptation}

\begin{frame}{Machine learning needs adaptation}
\small
\begin{tikzpicture}[data/.style={font=\scriptsize}]

\node[data, label={Source Domain}] (source-data) at (0, 0) {
\begin{tabular}{@{} l l @{}}
\toprule
Input & Output \\
\midrule
Dec 5, 2007 & 2007-12-05 \\
3 Jan 2018 & 2018-01-03 \\
04-25-1990 & 1990-04-25 \\
\ldots & \ldots \\
\bottomrule
\end{tabular}
};

\node[label={Learned Model}, inner sep=0pt] (model) at (5, 0) {\includegraphics{Hey_Machine_Learning_Logo.png}};

\draw[ultra thick, -latex]  (source-data)  -- node[align=center] {Machine \\ Learning} (model);

\visible<2->{
\node[data, label={Target Domain}, anchor=south west] (target-data) at (7, 0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Input \\
\midrule
3 weeks postoperative \\
1 tablet q.i.d \\
75-year-old \\
\ldots \\
\bottomrule
\end{tabular}
};
\draw[ultra thick, -latex]  (target-data.south west)  -- (model.east);
}

\visible<3->{
\node[data, anchor=north west] (output) at (7, -0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Output \\
\midrule
??? \\
??? \\
??? \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (model.east)  -- (output.north west);
}
\end{tikzpicture}
\end{frame}

\begin{frame}{Machine learning needs adaptation}
A sample of recent NLP models, trained on a source domain, and tested on a target domain:

\bigskip
\footnotesize
\begin{tabular}{ l l l l l }
\toprule
& \multicolumn{2}{c}{Source} & \multicolumn{2}{c}{Target} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
Task & Domain & Score & Domain & Score \\
\midrule
Sentences & Mayo Clinic & 94.9 A & BIDMC ICU & 31.8 A \\
Syntax trees & Wall Street Journal & 89 $F_1$ & Medline abstracts & 75 $F_1$ \\
Negations & Partners Healthcare & 93.6 $F_1$ & Mayo Clinic & 74.7 $F_1$ \\
Times & Mayo colon cancer & 80 $F_1$ & Mayo brain cancer & 59 $F_1$ \\
\ldots \\
\bottomrule
\end{tabular}

\centering
\smallskip
Full table in \cite{laparra:JAMIAOpen:2020}
\end{frame}

\begin{frame}{Domain adaptive machine learning}
\small
\begin{tikzpicture}[data/.style={font=\scriptsize}]

\node[data, label={Source Domain}] (source-data) at (0, 2) {
\begin{tabular}{@{} l l @{}}
\toprule
Input & Output \\
\midrule
Dec 5, 2007 & 2007-12-05 \\
3 Jan 2018 & 2018-01-03 \\
04-25-1990 & 1990-04-25 \\
\ldots & \ldots \\
\bottomrule
\end{tabular}
};

\node[data, label={Target Domain}] (target-data) at (0, -2) {
\begin{tabular}{@{} l l @{}}
\toprule
Input & \color{lightgray} Output\\
\midrule
1 week postop &  \color{lightgray} 2011-07-06 \\
2 tablets b.i.d. &  \color{lightgray} 2$\times$2$\times$P1D \\
40-year-old &  \color{lightgray} P40Y \\
\ldots \\
\bottomrule
\end{tabular}
};

\node[label={Learned Model}, inner sep=0pt] (model) at (5, 0) {\includegraphics{Hey_Machine_Learning_Logo.png}};

\node[align=center] at (2.8, 0) {Machine \\ Learning};

\draw[ultra thick, -latex]  (source-data)  -- (model);
\draw[ultra thick, -latex]  (target-data)  -- (model);

\visible<2->{\node[data, anchor=south west] (input) at (7, 0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Input \\
\midrule
3 weeks postoperative \\
1 tablet q.i.d. \\
75-year-old \\
\ldots \\
\bottomrule
\end{tabular}
};

\node[data, anchor=north west] (output) at (7, -0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Output \\
\midrule
2011-07-20 \\
1$\times$4$\times$P1D \\
P70Y \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (input.south west)  -- (model.east);
\draw[ultra thick, -latex]  (model.east)  -- (output.north west);
}
\end{tikzpicture}
\end{frame}

\begin{frame}{Domain adaptation is hard}
New label distributions for known phrases
\begin{itemize}
\item E.g., the word \textit{day}
\begin{itemize}
\item in news text is 68\% \textsc{Period}, 32\% \textsc{Calendar-Interval}
\item in clinical text is 6\% \textsc{Period}, 94\% \textsc{Calendar-Interval}
\end{itemize}
\end{itemize}

\pause
\bigskip
New phrases for known concepts
\begin{itemize}
\item E.g., \textit{q.i.d} meaning \textit{4 times per day}
\end{itemize}

\pause
\bigskip
New concepts
\begin{itemize}
\item E.g., \textit{lean season} or \textit{tef season}
\end{itemize}
\end{frame}

\begin{frame}{Clinical domain adaptation is even harder}
\begin{itemize}
\item Doctors notes vary wildly in format, style, and word choice depending on both medical institution and medical specialty
\begin{itemize}
\item E.g., for years cTAKES assumed newlines ended sentences because this was true in the Mayo Clinic EMR
\end{itemize}
\pause
\item Data is hard to share, due to protected health information
\begin{itemize}
\item IRB approvals and data use agreements can be complex
\item Previously approved data cannot be distributed without sustained funding (e.g., the Mayo Clinic's THYME corpus)
\item Many annotated datasets will never leave their original institution in any form
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

\section{Domain adaptation comes in many forms}

\begin{frame}{Taxonomy of domain adaptation}
\begin{tabular}{ l l l }
\toprule
Souce shares & Target has & Example approach \\
\midrule
\pause
Labeled text & Labeled text & Feature augmentation \\
\pause
Labeled text & Raw text & Self-training \\
\pause
Trained models & Labeled text & Fine-tuning \\
\pause
Trained models & Raw text & ??? \\
\bottomrule
\end{tabular}
\end{frame}

\begin{frame}{Feature augmentation \hfill\small e.g., \cite{daume-iii-2007-frustratingly}}
\begin{tikzpicture}[
  font=\small,
  data/.style={font=\footnotesize, inner sep=0pt},
  doc/.style={
    draw, minimum height=3em, minimum width=2em, fill=white,
    double copy shadow={shadow xshift=-4pt, shadow yshift=-4pt, fill=white, draw}
  },
]

\node[doc,label={Source Domain}] (source-docs) at (-4, 1.5) {+1};

\node[data] (source-features) at (0, 1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c >{\color{ua-red}}c >{\color{ua-oasis}}c c @{}}
\toprule
Features & Source & Target & Label \\
\midrule
0 1 0 0 & 0 1 0 0 & 0 0 0 0 & +1\\
1 0 1 0 & 1 0 1 0 & 0 0 0 0 & -1\\
\ldots \\
\bottomrule
\end{tabular}
};

\node[doc,label={Target Domain}] (target-docs) at (-4, -1.5) {-1};

\node[data] (target-features) at (0, -1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c >{\color{ua-red}}c >{\color{ua-oasis}}c c @{}}
\toprule
Features & Source & Target & Label \\
\midrule
1 1 1 0 & 0 0 0 0 & 1 1 1 0 & -1 \\
0 0 1 1 & 0 0 0 0 & 0 0 1 1 & +1 \\
\ldots \\
\bottomrule
\end{tabular}
};

\node[label={Learned Model}, inner sep=0pt] (model) at (5, 0) {\includegraphics{Hey_Machine_Learning_Logo.png}};

\draw[ultra thick, -latex]  (source-docs) -- (source-features);
\draw[ultra thick, -latex]  (source-features.east)  -- (model);
\draw[ultra thick, -latex]  (target-docs) -- (target-features);
\draw[ultra thick, -latex]  (target-features.east)  -- (model);

\end{tikzpicture}

Assumptions:
\begin{itemize}
\item Source shares labeled text
\item Target has (small amounts of) labeled text
\end{itemize}
\end{frame}

\begin{frame}{Self-training \hfill\small e.g., \cite{yarowsky-1995-unsupervised,ruder-plank-2018-strong}}
% maybe the tri-training paper?
% diagram of method
% why it doesn't work for Clinical NLP
\begin{tikzpicture}[
  font=\small,
  data/.style={font=\footnotesize, inner sep=0pt},
  doc/.style={
    draw, minimum height=3em, minimum width=2em, fill=white,
    double copy shadow={shadow xshift=-4pt, shadow yshift=-4pt, fill=white, draw}
  },
]

\node[doc,label={Source Domain}] (source-docs) at (-2, 1.5) {+1};

\node[data,anchor=west] (source-features) at (0, 1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c c @{}}
\toprule
Features & Label \\
\midrule
0 1 0 0 & +1\\
1 0 1 0 & -1\\
\ldots \\
\bottomrule
\end{tabular}
};

\node[label={Learned Model}, inner sep=0pt] (model) at (6, 1.5) {\includegraphics[scale=0.8]{Hey_Machine_Learning_Logo.png}};

\draw[ultra thick, -latex]  (source-docs) -- (source-features);
\draw[ultra thick, -latex]  (source-features.east)  -- (model);

\node[doc,label={Target Domain}] (target-docs) at (-2, -1.5) {?};

\visible<2->{
\node[data,anchor=west] (target-features) at (0, -1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c @{}}
\toprule
Features  \\
\midrule
1 1 1 0 \\
0 0 1 1 \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (target-docs) -- (target-features);
\draw[ultra thick, -latex]  (target-features.north)  -- (model);
}

\visible<3->{
\node[data] (target-labels) at (2.5, -1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c @{}}
\toprule
Labels  \\
\midrule
+1 \\
+1 \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (model)  -- (target-labels.north);
}

\visible<4->{
\node[label={Self-trained Model}, inner sep=0pt] (st-model) at (6, -1.5) {\includegraphics[scale=0.8]{Hey_Machine_Learning_Logo.png}};

\draw [draw, thick] (-0.3,-2.8) rectangle (3.4,2.8);
\draw[ultra thick, -latex]  (3.4, -1.5)  -- (st-model);
}
\end{tikzpicture}

Assumptions:
\begin{itemize}
\item Source shares labeled text
\item Target has raw text
\end{itemize}
\end{frame}

\section{Domain adaptation for clinical information extraction}

\begin{frame}{Source-free domain adaptation}
% summarize shared tasks: negation and time
\end{frame}

\begin{frame}{Source-free self-training}
% Xin's results
\end{frame}

\begin{frame}{Online active learning}
% any results from Xin
\end{frame}

\section{Domain adaptation for clinical concept normalization}

\begin{frame}{Clinical concept normalization}
\end{frame}

\begin{frame}{Normalization as vector-space search}
\end{frame}

\begin{frame}{Domain adaptation without retraining}
\end{frame}

\section*{Summary and future work}

\begin{frame}{Summary}
\end{frame}

\begin{frame}{Future work}
\end{frame}

\appendix

\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{apalike}
        \bibliography{domain-adaptation.bib}
\end{frame}


\end{document}
