% !TEX TS-program = pdflatexmk

\documentclass[14pt]{beamer}
\usepackage{newtxtext,newtxmath}
\usepackage{microtype}
\usepackage[english]{babel}
\usepackage{array}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shadows}
\usetikzlibrary{shapes}
\usetikzlibrary{shapes.multipart}


% Define UA colors
% https://brand.arizona.edu/applying-the-brand/colors
\definecolor{ua-red}{HTML}{AB0520}
\definecolor{ua-blue}{HTML}{0C234B}
\definecolor{ua-oasis}{HTML}{378DBD}

\mode<presentation>{
\usetheme{Madrid}
\usecolortheme[named=ua-red]{structure}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{subsection in toc}[square]
\setbeamertemplate{items}[square]
\setbeamercovered{transparent=0}
}

\tikzset{
  doc/.style={
    draw, minimum height=3em, minimum width=2em, fill=white,
    double copy shadow={shadow xshift=-4pt, shadow yshift=-4pt, fill=white, draw}
  },
  ml model/.style={
    node contents={\includegraphics[scale=#1]{Hey_Machine_Learning_Logo.png}},
    inner sep=0pt,
  },
  ml model/.default={1},
}

% command for annotating words in text
% #1: drawing options
% #2: name of entire shape
% #3: number of parts
% #4: name of baseline part
% #5: annotation type
% #6: node text following \nodepart{two}
\newcommand{\annotate}[6][]{%
\tikz[remember picture,baseline={(#2.#4)}]{%
\node[
  rectangle split,
  rectangle split parts=#3,
  thick,
  inner sep=2pt,
  align=center,
  draw,
  #1]%
(#2)
{%
\scriptsize\strut\textsc{#5}%
\nodepart{two}%
#6%
\strut};}}

\newcommand{\AnnotateTwo}[4][]{\annotate[#1]{#2}{2}{two}{#3}{#4}}


\author[Bethard]{Dr. Steven Bethard}
\institute[Arizona]{%
School of Information\\
University of Arizona}

\title{Adapting natural language processing models across clinical domains}
\date[]{26 Feb 2021}


\begin{document}


\begin{frame}
  \titlepage
\end{frame}

\section{Machine learning needs domain adaptation}

\begin{frame}{Machine learning needs adaptation}
\small
\begin{tikzpicture}[data/.style={font=\scriptsize}]

\node[data, label={Source Domain}] (source-data) at (0, 0) {
\begin{tabular}{@{} l l @{}}
\toprule
Input & Output \\
\midrule
Dec 5, 2007 & 2007-12-05 \\
3 Jan 2018 & 2018-01-03 \\
04-25-1990 & 1990-04-25 \\
\ldots & \ldots \\
\bottomrule
\end{tabular}
};

\node (model) at (5, 0) [ml model, label={Learned Model}];

\draw[ultra thick, -latex]  (source-data)  -- node[align=center] {Machine \\ Learning} (model);

\visible<2->{
\node[data, label={Target Domain}, anchor=south west] (target-data) at (7, 0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Input \\
\midrule
3 weeks postoperative \\
1 tablet q.i.d \\
75-year-old \\
\ldots \\
\bottomrule
\end{tabular}
};
\draw[ultra thick, -latex]  (target-data.south west)  -- (model.east);
}

\visible<3->{
\node[data, anchor=north west] (output) at (7, -0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Output \\
\midrule
??? \\
??? \\
??? \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (model.east)  -- (output.north west);
}
\end{tikzpicture}
\end{frame}

\begin{frame}{Machine learning needs adaptation}
A sample of recent NLP models, trained on a source domain, and tested on a target domain:

\bigskip
\footnotesize
\begin{tabular}{ l l l l l }
\toprule
& \multicolumn{2}{c}{Source} & \multicolumn{2}{c}{Target} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
Task & Domain & Score & Domain & Score \\
\midrule
Sentences & Mayo Clinic & 94.9 A & BIDMC ICU & 31.8 A \\
Syntax trees & Wall Street Journal & 89 $F_1$ & Medline abstracts & 75 $F_1$ \\
Negations & Partners Healthcare & 93.6 $F_1$ & Mayo Clinic & 74.7 $F_1$ \\
Times & Mayo colon cancer & 80 $F_1$ & Mayo brain cancer & 59 $F_1$ \\
\ldots \\
\bottomrule
\end{tabular}

\centering
\smallskip
Full table in \cite{laparra:JAMIAOpen:2020}
\end{frame}

\begin{frame}{Domain adaptive machine learning}
\small
\begin{tikzpicture}[data/.style={font=\scriptsize}]

\node[data, label={Source Domain}] (source-data) at (0, 2) {
\begin{tabular}{@{} l l @{}}
\toprule
Input & Output \\
\midrule
Dec 5, 2007 & 2007-12-05 \\
3 Jan 2018 & 2018-01-03 \\
04-25-1990 & 1990-04-25 \\
\ldots & \ldots \\
\bottomrule
\end{tabular}
};

\node[data, label={Target Domain}] (target-data) at (0, -2) {
\begin{tabular}{@{} l l @{}}
\toprule
Input & \color{lightgray} Output\\
\midrule
1 week postop &  \color{lightgray} 2011-07-06 \\
2 tablets b.i.d. &  \color{lightgray} 2$\times$2$\times$P1D \\
40-year-old &  \color{lightgray} P40Y \\
\ldots \\
\bottomrule
\end{tabular}
};

\node (model) at (5, 0)  [ml model, label={Learned Model}];

\node[align=center] at (2.8, 0) {Machine \\ Learning};

\draw[ultra thick, -latex]  (source-data)  -- (model);
\draw[ultra thick, -latex]  (target-data)  -- (model);

\visible<2->{\node[data, anchor=south west] (input) at (7, 0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Input \\
\midrule
3 weeks postoperative \\
1 tablet q.i.d. \\
75-year-old \\
\ldots \\
\bottomrule
\end{tabular}
};

\node[data, anchor=north west] (output) at (7, -0.5) {
\begin{tabular}{@{} p{9em} @{}}
\toprule
Output \\
\midrule
2011-07-20 \\
1$\times$4$\times$P1D \\
P70Y \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (input.south west)  -- (model.east);
\draw[ultra thick, -latex]  (model.east)  -- (output.north west);
}
\end{tikzpicture}
\end{frame}

\begin{frame}{Domain adaptation is hard}
New label distributions for known phrases
\begin{itemize}
\item E.g., the word \textit{day}
\begin{itemize}
\item in news text is 68\% \textsc{Period}, 32\% \textsc{Calendar-Interval}
\item in clinical text is 6\% \textsc{Period}, 94\% \textsc{Calendar-Interval}
\end{itemize}
\end{itemize}

\pause
\bigskip
New phrases for known concepts
\begin{itemize}
\item E.g., \textit{q.i.d} meaning \textit{4 times per day}
\end{itemize}

\pause
\bigskip
New concepts
\begin{itemize}
\item E.g., \textit{lean season} or \textit{tef season}
\end{itemize}
\end{frame}

\begin{frame}{Clinical domain adaptation is even harder}
\begin{itemize}
\item Doctors notes vary wildly in format, style, and word choice depending on both medical institution and medical specialty
\begin{itemize}
\item E.g., for years cTAKES assumed newlines ended sentences because this was true in the Mayo Clinic EMR
\end{itemize}
\pause
\item Data is hard to share, due to protected health information
\begin{itemize}
\item IRB approvals and data use agreements can be complex
\item Previously approved data cannot be distributed without sustained funding (e.g., the Mayo Clinic's THYME corpus)
\item Many annotated datasets will never leave their original institution in any form
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

\section{Domain adaptation comes in many forms}

\begin{frame}{Taxonomy of domain adaptation}
\begin{tabular}{ l l l }
\toprule
Souce shares & Target has & Example approach \\
\midrule
\pause
Labeled text & Labeled text & Feature augmentation \\
\pause
Labeled text & Raw text & Self-training \\
\pause
Trained models & Labeled text & Fine-tuning \\
\pause
Trained models & Raw text & ??? \\
\bottomrule
\end{tabular}
\end{frame}

\begin{frame}{Feature augmentation \hfill\small e.g., \cite{daume-iii-2007-frustratingly}}
\begin{tikzpicture}[
  font=\small,
  data/.style={font=\footnotesize, inner sep=0pt},
]

\node[doc,label={Source Domain}] (source-docs) at (-4, 1.5) {+1};

\node[data] (source-features) at (0, 1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c >{\color{ua-red}}c >{\color{ua-oasis}}c c @{}}
\toprule
Features & Source & Target & Label \\
\midrule
0 1 0 0 & 0 1 0 0 & 0 0 0 0 & +1\\
1 0 1 0 & 1 0 1 0 & 0 0 0 0 & -1\\
\ldots \\
\bottomrule
\end{tabular}
};

\node[doc,label={Target Domain}] (target-docs) at (-4, -1.5) {-1};

\node[data] (target-features) at (0, -1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c >{\color{ua-red}}c >{\color{ua-oasis}}c c @{}}
\toprule
Features & Source & Target & Label \\
\midrule
1 1 1 0 & 0 0 0 0 & 1 1 1 0 & -1 \\
0 0 1 1 & 0 0 0 0 & 0 0 1 1 & +1 \\
\ldots \\
\bottomrule
\end{tabular}
};

\node (model) at (5, 0) [ml model=0.8, label={Learned Model}];

\draw[ultra thick, -latex]  (source-docs) -- (source-features);
\draw[ultra thick, -latex]  (source-features.east)  -- (model);
\draw[ultra thick, -latex]  (target-docs) -- (target-features);
\draw[ultra thick, -latex]  (target-features.east)  -- (model);

\end{tikzpicture}

Assumptions:
\begin{itemize}
\item Source shares labeled text
\item Target has (small amounts of) labeled text
\end{itemize}
\end{frame}

\begin{frame}{Self-training \hfill\small e.g., \cite{yarowsky-1995-unsupervised,ruder-plank-2018-strong}}
\begin{tikzpicture}[
  font=\small,
  data/.style={font=\footnotesize, inner sep=0pt},
]

\node[doc,label={Source Domain}] (source-docs) at (-2, 1.5) {+1};

\node[data,anchor=west] (source-features) at (0, 1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c c @{}}
\toprule
Features & Label \\
\midrule
0 1 0 0 & +1\\
1 0 1 0 & -1\\
\ldots \\
\bottomrule
\end{tabular}
};

\node (model) at (6, 1.5) [ml model=0.8, label={Learned Model}];

\draw[ultra thick, -latex]  (source-docs) -- (source-features);
\draw[ultra thick, -latex]  (source-features.east)  -- (model);

\node[doc,label={Target Domain}] (target-docs) at (-2, -1.5) {?};

\visible<2->{
\node[data,anchor=west] (target-features) at (0, -1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c @{}}
\toprule
Features  \\
\midrule
1 1 1 0 \\
0 0 1 1 \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (target-docs) -- (target-features);
\draw[ultra thick, -latex]  (target-features.north)  -- (model);
}

\visible<3->{
\node[data] (target-labels) at (2.5, -1.5) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c @{}}
\toprule
Labels  \\
\midrule
+1 \\
+1 \\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (model)  -- (target-labels.north);
}

\visible<4->{
\node (st-model) at (6, -1.5)  [ml model=0.8, label={Self-trained Model}];

\draw [draw, thick] (-0.3,-2.8) rectangle (3.4,2.8);
\draw[ultra thick, -latex]  (3.4, -1.5)  -- (st-model);
}
\end{tikzpicture}

Assumptions:
\begin{itemize}
\item Source shares labeled text
\item Target has raw text
\end{itemize}
\end{frame}


\begin{frame}{Fine-tuning \hfill\small e.g., \cite{yosinski:NIPS:2014,lee-etal-2018-transfer}}
\begin{tikzpicture}[
  font=\small,
  data/.style={font=\footnotesize, inner sep=0pt},
]

\node[doc,label={Source Domain}] (source-docs) at (-2, 1.4) {+1};

\node[data,anchor=west] (source-features) at (0, 1.4) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c c @{}}
\toprule
Features & Label \\
\midrule
0 1 0 0 & +1\\
1 0 1 0 & -1\\
\ldots \\
\bottomrule
\end{tabular}
};

\node (model) at (6, 1.4)  [ml model=0.8, label={Learned Model}];

\draw[ultra thick, -latex]  (source-docs) -- (source-features);
\draw[ultra thick, -latex]  (source-features.east)  -- (model);

\visible<2->{
\node[doc,label={Target Domain}] (target-docs) at (-2, -1.4) {-1};

\node[data,anchor=west] (target-features) at (0, -1.4) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c c @{}}
\toprule
Features & Label \\
\midrule
1 1 1 0 & -1 \\
0 0 1 1 & +1\\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (target-docs) -- (target-features);

\node (ft-model) at (6, -1.4) [ml model=0.8, label=below:{Fine-tuned Model}];

\draw[ultra thick, -latex]  (model)  -- (ft-model);
\draw[ultra thick, -latex]  (target-features)  -- (ft-model);
}
\end{tikzpicture}

\begin{columns}
\begin{column}{0.46\textwidth}
Assumptions:
\begin{itemize}
\item Source shares model
\item Target has labeled text
\end{itemize}
\end{column}
\begin{column}<3->{0.53\textwidth}
Dangers:
\begin{itemize}
\item Catastrophic forgetting \small\cite{mccloskey:PLM:1989}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\section{Domain adaptation for clinical information extraction}

\begin{frame}{SemEval-2021 Task 10}{Source-Free Domain Adaptation for Semantic Processing}
\begin{tikzpicture}[
  font=\small,
  data/.style={font=\footnotesize, inner sep=0pt},
]

\node[doc,label={Source Domain}] (source-docs) at (-3, 1.4) {+1};

\node[data] (source-features) at (0, 1.4) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c c @{}}
\toprule
Features & Label \\
\midrule
0 1 0 0 & +1\\
1 0 1 0 & -1\\
\ldots \\
\bottomrule
\end{tabular}
};

\node (provided-model) at (3.5, 1.4)  [ml model=0.8, label={Provided Model}];

\draw[ultra thick, -latex]  (source-docs) -- (source-features);
\draw[ultra thick, -latex]  (source-features.east)  -- (provided-model);

\node[doc,label={Target Domain}] (target-docs) at (-3, -1.4) {?};

\node[draw,cloud,aspect=2,ua-red,ultra thick] (participant) at (0, -1.4) {Participant};

\draw[ultra thick, -latex]  (target-docs) -- (participant);
\draw[ultra thick, -latex]  (provided-model) -- (participant);

\node (adapted-model) at (3.5, -1.4)  [ml model=0.8, label={Adapted Model}];

\draw[ultra thick, -latex]  (participant) -- (adapted-model);

\node[data,anchor=west] (target-labels) at (5.5, -1.4) {
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{} c @{}}
\toprule
Label \\
\midrule
-1 \\
+1\\
\ldots \\
\bottomrule
\end{tabular}
};

\draw[ultra thick, -latex]  (adapted-model) -- (target-labels);
\end{tikzpicture}
\end{frame}

\begin{frame}{SemEval-2021 Task 10}{Source-Free Domain Adaptation for Semantic Processing}
\begin{columns}
\begin{column}{0.45\textwidth}
\begin{block}{Task: Negation}
\small
Has frequent \AnnotateTwo{diarrhea}{}{diarrhea}, but no new \AnnotateTwo{lumps}{Negated}{lumps} or \AnnotateTwo{masses}{Negated}{masses}\ldots
\end{block}
\end{column}
\begin{column}{0.5\textwidth}
\begin{block}{Task: Time Expressions}
\small
In \AnnotateTwo{month}{Month-Of-Year}{January} of \AnnotateTwo{year}{Year}{2009}, she had pain \AnnotateTwo{number}{Number}{four} \AnnotateTwo{period}{Period}{hours} \AnnotateTwo{after}{After}{after}\ldots
\end{block}
\end{column}
\end{columns}

\pause
\bigskip
\begin{tabular}{@{} l l l @{}}
\toprule
& Negation & Time Expressions  \\
\midrule
Trained model & Mayo Clinic & Mayo Clinic \\
Dev data & Partners HealthCare & News \\
Test data & Beth Israel ICU & Food Insecurity \\
\bottomrule
\end{tabular}
\end{frame}

\begin{frame}{SemEval-2021 Task 10}{Source-Free Domain Adaptation for Semantic Processing}


\begin{tabular}{ l c c c c c c }
\toprule
& \multicolumn{3}{c}{Negation} & \multicolumn{3}{c}{Time Expressions} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
System & P & R & $F_1$ & P & R & $F_1$ \\
\midrule
Top participant & 90.2 & 75.6 & \alert<3>{82.2} & 84.7 & 78.5 & \alert<3>{81.5} \\
Fine-tune on dev & 90.8 & 61.1 & \alert<2-3>{73.0} & 82.7 & 78.2 & \alert<2-3>{80.4} \\
Provided model & 91.7 & 51.6 & \alert<2>{66.0} & 84.9 & 74.6 & \alert<2>{79.4} \\
\bottomrule
\end{tabular}

\bigskip
Findings:
\begin{itemize}
\item<2-> Fine-tuning on dev domain helps; no catastrophic forgetting, and despite test domain being different
\item<3-> Participant systems made larger gains on negation than time expressions
\end{itemize}
\end{frame}

\begin{frame}{Source-free self-training}
% Xin's results
\end{frame}

\begin{frame}{Online active learning}
% any results from Xin
\end{frame}

\section{Domain adaptation for clinical concept normalization}

\begin{frame}{Clinical concept normalization}
\end{frame}

\begin{frame}{Normalization as vector-space search}
\end{frame}

\begin{frame}{Domain adaptation without retraining}
\end{frame}

\section*{Summary and future work}

\begin{frame}{Summary}
\end{frame}

\begin{frame}{Future work}
\end{frame}

\appendix

\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{apalike}
        \bibliography{domain-adaptation.bib}
\end{frame}


\end{document}
